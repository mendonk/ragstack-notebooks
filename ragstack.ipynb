{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGStack and Astra vector db\n",
    "\n",
    "This notebook demonstrates a RAG pattern using RAGStack and the AstraDB vector database.\n",
    "\n",
    "The pattern is:\n",
    "\n",
    "1. Construct information base\n",
    "2. Basic retrieval\n",
    "3. Generation with augmented context\n",
    "4. Advanced retrieval and generation\n",
    "5. Evaluate quality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "RAGStack includes all the libraries you need for the RAG pattern, including the vector database, embeddings pipeline, and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install ragstack-ai datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from datasets import load_dataset\n",
    "from langchain.vectorstores.astradb import AstraDB \n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have a vector database, create one at https://astra.datastax.com/.\n",
    "\n",
    "The Astra application token must have Database Administrator permissions (e.g. `AstraCS:WSnyFUhRxsrg…`​).\n",
    "\n",
    "The Astra API endpoint is available in the Astra Portal (e.g. `https://<ASTRA_DB_ID>-<ASTRA_DB_REGION>.apps.astra.datastax.com`).\n",
    "\n",
    "Create an OpenAI key at https://platform.openai.com/ (e.g. `sk-xxxx`).\n",
    "\n",
    "You must have an existing collection in Astra (e.g. `my-collection`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astra_token = getpass.getpass(\"Astra token:\")\n",
    "astra_endpoint = getpass.getpass(\"Astra db endpoint:\")\n",
    "openai_key = getpass.getpass(\"OpenAI Key:\")\n",
    "collection = input(\"Collection name:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG workflow\n",
    "\n",
    "With your environment set up, you're ready to create a RAG workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct information base\n",
    "\n",
    "Declare the embeddings model define its required parameters.\n",
    "Create your Astra vector database and configure its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings(openai_api_key=openai_key)\n",
    "vstore = AstraDB(\n",
    "        collection_name=collection,\n",
    "        embedding=embedding,\n",
    "        token=astra_token,\n",
    "        api_endpoint=astra_endpoint\n",
    "    )\n",
    "print(\"Astra configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a small dataset of quotes with the Python dataset module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_dataset = load_dataset(\"datastax/philosopher-quotes\")[\"train\"]\n",
    "print(\"An example entry:\")\n",
    "print(philo_dataset[16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process metadata and convert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for entry in philo_dataset:\n",
    "    metadata = {\"author\": entry[\"author\"]}\n",
    "    if entry[\"tags\"]:\n",
    "        # Add metadata tags to the metadata dictionary\n",
    "        for tag in entry[\"tags\"].split(\";\"):\n",
    "            metadata[tag] = \"y\"\n",
    "    # Add a LangChain document with the quote and metadata tags\n",
    "    doc = Document(page_content=entry[\"quote\"], metadata=metadata)\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inserted_ids = vstore.add_documents(docs)\n",
    "print(f\"\\nInserted {len(inserted_ids)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm your vector store is populated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vstore.astra_db.collection(collection).find())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic retrieval\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve context from your vector database, and pass it to OpenAI with a prompt question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "retriever = vstore.as_retriever(search_kwargs={'k': 3})\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Answer the question based only on the supplied context. If you don't know the answer, say you don't know the answer.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Your answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "model = ChatOpenAI(openai_api_key=openai_key)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(\"In the given context, what subject are philosophers most concerned with?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
